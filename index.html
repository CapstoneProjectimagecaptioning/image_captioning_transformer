<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Captioning using Transformer Models</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

    <header>
        <div class="container">
            <h1>üñºÔ∏è Image Captioning using Transformer Models üß†</h1>
            <p>Transforming how machines understand images and generate captions</p>
        </div>
    </header>

    <section id="overview" class="section">
        <div class="container">
            <h2>Project Overview</h2>
            <p>
                This project explores the use of transformer-based models for generating captions for images. Traditional CNN+RNN models are being replaced by state-of-the-art transformer architectures like BLIP (Bootstrapped Language-Image Pretraining) to achieve better results in visual-language tasks.
            </p>
        </div>
    </section>

    <section id="data" class="section">
        <div class="container">
            <h2>Data Sources</h2>
            <p>
                The <a href="https://cocodataset.org/#download">MS COCO dataset</a> is used to train and evaluate the model. It provides over 120,000 images with human-generated captions, making it an excellent dataset for training deep learning models for captioning tasks.
            </p>
        </div>
    </section>

    <section id="models" class="section">
        <div class="container">
            <h2>Models Used</h2>
            <ul>
                <li><strong>BLIP (Bootstrapped Language-Image Pretraining):</strong> A transformer-based model optimized for captioning and vision-language tasks.</li>
                <li><strong>Vision Transformer (ViT):</strong> Another state-of-the-art model for visual tasks, used for enhancing captioning accuracy.</li>
            </ul>
        </div>
    </section>

    <section id="metrics" class="section">
        <div class="container">
            <h2>Evaluation Metrics</h2>
            <ul>
                <li><strong>BLEU Score:</strong> Measures n-gram precision between the generated and reference captions.</li>
                <li><strong>CIDEr Score:</strong> Focuses on the consensus between human-generated captions and machine-generated captions.</li>
            </ul>
        </div>
    </section>

    <section id="results" class="section">
        <div class="container">
            <h2>Results</h2>
            <p>
                The transformer-based models performed significantly better in generating human-like captions compared to traditional models. Some of the key evaluation scores include:
            </p>
            <ul>
                <li><strong>BLEU Score:</strong> 0.75</li>
                <li><strong>CIDEr Score:</strong> 1.05</li>
            </ul>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2024 Image Captioning using Transformer Models | Designed by [Your Name]</p>
        </div>
    </footer>

</body>
</html>