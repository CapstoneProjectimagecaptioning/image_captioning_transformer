# üñºÔ∏è Image Captioning using Transformer Models üß†

![Project Banner](https://www.google.com/url?sa=i&url=https%3A%2F%2Fcv-tricks.com%2Fartificial-intelligence%2Fshow-attend-tell-image-captioning-explained%2F&psig=AOvVaw097dHYGnqR1AJSqTMP5u8y&ust=1729728439079000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCNCfq9uao4kDFQAAAAAdAAAAABAR)

## üìã Table of Contents
1. [Project Overview](#project-overview)
2. [Data Sources](#data-sources)
3. [Models Used](#models-used)
4. [Evaluation Metrics](#evaluation-metrics)
5. [Installation](#installation)
6. [How to Use](#how-to-use)
7. [Results](#results)
8. [Contributors](#contributors)
9. [License](#license)

## üìñ Project Overview

This project aims to implement an **Image Captioning System** using **transformer-based models**. Unlike traditional approaches using CNN + RNN architectures, we leverage transformers to generate more accurate and contextually aware captions for images. Our implementation builds on models like **BLIP (Bootstrapped Language-Image Pretraining)** and other **transformer-based architectures** to generate captions.

We use the **MS COCO Dataset** for training and evaluation, which provides a large-scale dataset of images and human-generated captions. This project compares the performance of transformers with older models, exploring their effectiveness in visual-language tasks.

## üìä Data Sources

For this project, we use the **MS COCO dataset**, which consists of over 80,000 training images, 40,000 validation images, and their corresponding captions.

- **Dataset Link**: [MS COCO Dataset](https://cocodataset.org/#download)
- **Data License**: The dataset is freely available for academic research.

Dataset structure:
- **Training Set**: 80,000 images
- **Validation Set**: 40,000 images
- **Annotations**: Human-generated captions for each image

## üõ†Ô∏è Models Used

### 1. **BLIP (Bootstrapped Language-Image Pretraining)**
   - This transformer-based model improves image captioning through pretraining on vision-language tasks.
   - **Why BLIP?**: BLIP provides a robust framework for generating contextual captions, outperforming traditional models like CNN-RNN.

### 2. **Vision Transformer (ViT)**
   - A state-of-the-art transformer model designed for visual tasks.
   - **Why ViT?**: Transformers can handle longer dependencies in text, making them highly efficient for tasks like image captioning.

## üìê Evaluation Metrics

We use the following metrics to evaluate our image captioning models:
- **BLEU Score (Bilingual Evaluation Understudy)**: Measures the precision of n-grams between generated captions and reference captions.
- **CIDEr (Consensus-based Image Description Evaluation)**: Focuses on consensus captions and gives a more human-like evaluation of image captions.

## üöÄ Installation

Follow these steps to set up the project locally:

1. **Clone the repository**:
   ```bash
   git clone https://github.com/your-username/image-captioning-transformer.git
   cd image-captioning-transformer