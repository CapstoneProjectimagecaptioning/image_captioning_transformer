# Image Captioning using Transformer Model 
## Contributors: Premanth Alahari, Charan Gudivada


## üìã Table of Contents
1. [Project Overview](#project-overview)
2. [Data Sources](#data-sources)
3. [Models Used](#models-used)
4. [Evaluation Metrics](#evaluation-metrics)
5. [Installation](#installation)
6. [How to Use](#how-to-use)
7. [Results](#results)
8. [Contributors](#contributors)
9. [License](#license)

## üìñ Project Overview

This project aims to implement an **Image Captioning System** using **transformer-based models**. Unlike traditional approaches using CNN + RNN architectures, we leverage transformers to generate more accurate and contextually aware captions for images. Our implementation builds on models like **BLIP (Bootstrapped Language-Image Pretraining)** and other **transformer-based architectures** to generate captions.

We use the **MS COCO Dataset** for training and evaluation, which provides a large-scale dataset of images and human-generated captions. This project compares the performance of transformers with older models, exploring their effectiveness in visual-language tasks.

## üìä Data Sources

For this project, we use the **MS COCO dataset**, which consists of over 80,000 training images, 40,000 validation images, and their corresponding captions.

- **Dataset Link**: [MS COCO Dataset](https://cocodataset.org/#download)
- **Data License**: The dataset is freely available for academic research.

Download some specific data from here: http://cocodataset.org/#download (described below)
Under Annotations, download:

2017 Train/Val annotations [241MB] (extract captions_train2017.json and captions_val2017.json, and place at locations cocoapi/annotations/captions_train2014.json and cocoapi/annotations/captions_val2017.json, respectively)
2017 Testing Image info [1MB] (extract image_info_test2017s.json and place at location cocoapi/annotations/image_info_test2017.json)
Under Images, download:

2017 Train images [83K/13GB] (extract the train2017 folder and place at location cocoapi/images/train2017/)
2017 Val images [41K/6GB] (extract the val2017 folder and place at location cocoapi/images/val2017/)
2017 Test images [41K/6GB] (extract the test2017 folder and place at location cocoapi/images/test2017/)

## üõ†Ô∏è Models Used

In the context of Automated Image Captioning Using Generative AI: A Transformer-Based Approach, the following models are used:

### **1. InceptionV3 (Image Feature Extractor):**

	‚Ä¢	Purpose: To extract visual features from the input images.
	‚Ä¢	Overview: InceptionV3 is a deep convolutional neural network (CNN) architecture that has shown remarkable performance in image classification tasks. For the image captioning task, a pre-trained InceptionV3 model is employed to extract a feature vector from each image, representing high-level semantic information.
	‚Ä¢	Role: The feature vector generated by InceptionV3 acts as the input for the next stage, where the language model processes this information to generate captions. The visual feature extraction ensures the model captures the essential details of the image.

### **2. Transformer Model (Caption Generator):**

	‚Ä¢	Purpose: To generate textual captions based on the extracted image features.
	‚Ä¢	Overview: The Transformer architecture has proven to be highly effective for sequence-to-sequence tasks, originally designed for NLP but now applied to various other domains like image captioning. The key component of the Transformer is the self-attention mechanism, which allows the model to process the entire sequence of words simultaneously and capture long-range dependencies without relying on the sequential nature of previous models like LSTMs or GRUs.
	‚Ä¢	Role:
	‚Ä¢	The image feature vector is passed into the decoder part of the Transformer, which is responsible for generating the caption.
	‚Ä¢	The Transformer model here operates in an encoder-decoder setup where the encoder processes the image features, and the decoder generates the caption word-by-word.
	‚Ä¢	The self-attention mechanism ensures that the generated words maintain a coherent and context-aware structure.

### **3. Embedding Layer:**

	‚Ä¢	Purpose: To represent words as dense vectors in a continuous vector space.
	‚Ä¢	Overview: The embedding layer converts each word in the vocabulary into a lower-dimensional vector representation. This layer is crucial for allowing the model to learn semantic similarities between words and handle variable-length input sequences.
	‚Ä¢	Role: The embeddings are used in both the Transformer decoder and during training to convert the target caption sentences into a format suitable for processing by the model.

### **4. Positional Encoding (Part of Transformer):**

	‚Ä¢	Purpose: To encode positional information into the input sequences since the Transformer model processes inputs simultaneously rather than sequentially.
	‚Ä¢	Overview: Since the Transformer model does not have a built-in mechanism for understanding the order of words, positional encodings are added to the input embeddings. These encodings allow the model to incorporate word order into its understanding of the sequence.
	‚Ä¢	Role: Positional encodings are added to the embeddings of the words in the caption, enabling the Transformer to maintain a sense of sequential context when generating captions.

**Training Strategy:**

	‚Ä¢	The InceptionV3 model is pre-trained on ImageNet for image feature extraction and is kept either frozen or fine-tuned during training.
	‚Ä¢	The Transformer decoder is trained using the COCO dataset, where the model is fed image features and target captions to learn how to map the visual data to a coherent sentence.

These models work together to generate accurate, fluent, and diverse image captions, leveraging both computer vision and natural language generation techniques.

## üìê Evaluation Metrics

We use the following metrics to evaluate our image captioning models:
- **BLEU Score (Bilingual Evaluation Understudy)**: Measures the precision of n-grams between generated captions and reference captions.
- **CIDEr (Consensus-based Image Description Evaluation)**: Focuses on consensus captions and gives a more human-like evaluation of image captions.

## üöÄ Installation

Follow these steps to set up the project locally:

1. **Clone the repository**:
   ```bash
   git clone https://github.com/premanthcherry/image_captioning_transformer.git
   cd image-captioning-transformer